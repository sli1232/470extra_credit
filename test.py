# -*- coding: utf-8 -*-
"""470extra_credit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lH31-rTdmSxdydzv8hU8Ytrhkw975s8Q
"""
import numpy as np
import torch
import datasets
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
import evaluate
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"


datasets.config.DOWNLOADED_DATASETS_PATH = "/fs/nexus-scratch/shile/470extra_credit/data"



# parse command line arguments
import argparse
parser = argparse.ArgumentParser(description='Evaluate the model on the test set.')
parser.add_argument('--checkpoint', type=str, default="./results/checkpoint-600", help='Path to the model checkpoint.')
parser.add_argument('--tokenizer', type=str, default="./results/checkpoint-600", help='Path to the tokenizer.') 
parser.add_argument('--original', default=False, action='store_true', help='Whether to use the original model checkpoint and tokenizer.')
args = parser.parse_args()

# print all arguments
print(args)


if args.original:
    # Load the original model checkpoint and tokenizer
    model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
    tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
    print('Original model and tokenizer loaded successfully!')
else:
    # Load model checkpoint from local training
    model = AutoModelForSeq2SeqLM.from_pretrained(args.checkpoint)
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)
    print('Fine-tuned model and tokenizer loaded successfully!')


# Set source and target languages for tokenizer
tokenizer.src_lang = "en_XX"
tokenizer.tgt_lang = "zh_CN"

# Sanity check to ensure that the model can generate text
# Example English sentence
article_en = "The Secretary-General of the United Nations says there is no military solution in Syria."
encoded_en = tokenizer(article_en, return_tensors="pt")
generated_tokens = model.generate(
    **encoded_en,
    forced_bos_token_id=tokenizer.lang_code_to_id["zh_CN"]
)
decoded_tokens = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
assert len(decoded_tokens) == 1 and len(decoded_tokens[0]) > 0
print('Model translated text successfully!')

# Load the dataset
print("Loading dataset...")
dataset = datasets.load_dataset("shaowenchen/translation_zh")

# load the validation set as test set
test_dataset = dataset["validation"]
random_seed = 42
test_size = 1000
test_dataset = test_dataset.shuffle(seed=random_seed).select(range(test_size))

# Validate dataset to remove None values
def is_validate_example(example):
    return example["english"] is not None and example["chinese"] is not None

# Filter out examples with None values
test_dataset = test_dataset.filter(is_validate_example)


def preprocess_function(examples):
    return tokenizer.prepare_seq2seq_batch(
        src_texts=examples["english"],
        tgt_texts=examples["chinese"],
        max_length=256,
        max_target_length=128,
        padding="max_length",
        truncation=True,
    )

tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)


# Load the BLEU metric
metric = evaluate.load("bleu")

# Define the function to compute metrics
def compute_metrics(eval_pred, compute_result=False):
    logits, labels = eval_pred
    preds = logits[0].cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    # Replace -100 with the tokenizer's pad token ID to ignore padded tokens
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    metric.add_batch(predictions=decoded_preds, references=decoded_labels)
    if compute_result: # This is for the final evaluation
        # Compute BLEU score
        result = metric.compute()
        return {"bleu": result["bleu"]}

# Define the function to preprocess logits for metrics
def preprocess_logits_for_metrics(logits, labels):
    pred_ids = torch.argmax(logits[0], dim=-1)
    return pred_ids, labels


# Define the evaluation arguments
eval_args = TrainingArguments(
    per_device_eval_batch_size=20,
    batch_eval_metrics=True,
    output_dir="./results/evaluation",
)

# Define the trainer
trainer = Trainer(
    model=model,
    data_collator=data_collator,
    args=eval_args,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    preprocess_logits_for_metrics=preprocess_logits_for_metrics,
)

# Evaluate the model on the test set
print("Evaluating model on test set...")
results = trainer.evaluate(tokenized_test_dataset)

# Print the BLEU score
print(f"BLEU: {results['eval_bleu']}")
