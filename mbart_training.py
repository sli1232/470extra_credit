# -*- coding: utf-8 -*-
"""470extra_credit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lH31-rTdmSxdydzv8hU8Ytrhkw975s8Q
"""
import numpy as np
import torch
import datasets
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
import evaluate

datasets.config.DOWNLOADED_DATASETS_PATH = "/fs/nexus-scratch/shile/470extra_credit/data"


tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
print('Model and tokenizer loaded successfully!')

# Set source and target languages for tokenizer
tokenizer.src_lang = "en_XX"
tokenizer.tgt_lang = "zh_CN"

# Sanity check to ensure that the model can generate text
# Example English sentence
article_en = "The Secretary-General of the United Nations says there is no military solution in Syria."
encoded_en = tokenizer(article_en, return_tensors="pt")
generated_tokens = model.generate(
    **encoded_en,
    forced_bos_token_id=tokenizer.lang_code_to_id["zh_CN"]
)
decoded_tokens = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
assert len(decoded_tokens) == 1 and len(decoded_tokens[0]) > 0
print('Model translated text successfully!')

# Load the dataset
print("Loading dataset...")
dataset = datasets.load_dataset("shaowenchen/translation_zh")

# Split the dataset into training and validation
train_dataset = dataset["train"]
val_dataset = dataset["validation"]

# Print the first 3 examples from the training dataset:
print("Training dataset (first 3 examples):")
for i in range(3):
    print(train_dataset[i])
# Print the first 3 examples from the validation dataset:
print("Validation dataset (first 3 examples):")
for i in range(3):
    print(val_dataset[i])

# Validate dataset to remove None values
def is_validate_example(example):
    return example["english"] is not None and example["chinese"] is not None

# Filter out examples with None values
train_dataset = train_dataset.filter(is_validate_example)
val_dataset = val_dataset.filter(is_validate_example)

# Shuffle and select a subset of the data
random_seed = 42
train_size, val_size = 10000, 1000
raw_train_dataset = train_dataset.shuffle(seed=random_seed).select(range(train_size))
raw_val_dataset = val_dataset.shuffle(seed=random_seed).select(range(val_size))

def preprocess_function(examples):
    return tokenizer.prepare_seq2seq_batch(
        src_texts=examples["english"],
        tgt_texts=examples["chinese"],
        max_length=256,
        max_target_length=128,
        padding="max_length",
        truncation=True,
    )

# def preprocess_function(examples):
#     return tokenizer(
#         examples['english'],
#         text_target=examples['chinese'],
#         max_length=256,
#         # max_target_length=128,
#         padding="max_length",
#         truncation=True,
#     )

tokenized_train_dataset = raw_train_dataset.map(preprocess_function, batched=True)
tokenized_val_dataset = raw_val_dataset.map(preprocess_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)


training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=5e-5,  # Start with a small value
    per_device_train_batch_size=50,
    per_device_eval_batch_size=50,
    batch_eval_metrics=True,
    # gradient_accumulation_steps=2,
    # eval_accumulation_steps=2,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=10,
    report_to="none",
    load_best_model_at_end=True,
    metric_for_best_model="loss"
)

# Load the BLEU metric
metric = evaluate.load("bleu")

def compute_metrics(eval_pred, compute_result=False):
    logits, labels = eval_pred
    preds = logits[0].cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    # Replace -100 with the tokenizer's pad token ID to ignore padded tokens
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    metric.add_batch(predictions=decoded_preds, references=decoded_labels)
    if compute_result: # This is for the final evaluation
        # Compute BLEU score
        bleu = metric.compute()
        return {"bleu": bleu["score"]}
    
def preprocess_logits_for_metrics(logits, labels):
    pred_ids = torch.argmax(logits[0], dim=-1)
    return pred_ids, labels

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    preprocess_logits_for_metrics=preprocess_logits_for_metrics,
)

if torch.cuda.is_available():
    print("GPU is available!")
    print(f"Device Name: {torch.cuda.get_device_name(0)}")
    torch.cuda.empty_cache()
else:
    print("GPU is not available.")

trainer.train()
print("Training completed!")